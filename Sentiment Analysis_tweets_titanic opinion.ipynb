{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "happy = twitter_samples.strings('positive_tweets.json')\n",
    "print (len(happy)) \n",
    " \n",
    "sad = twitter_samples.strings('negative_tweets.json')\n",
    "print (len(sad)) \n",
    "\n",
    "nane = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "print (len(nane))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "sad_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "nane_tokens = twitter_samples.tokenized('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    " \n",
    "#Positive smiles\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# negative smiles\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all smiles\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation du modéle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "# order the features positive and negative \n",
    "def bag_of_words(tweet):\n",
    "    words = clean_tweets(tweet)\n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary\n",
    " \n",
    "\n",
    "#all positive tweets in one list\n",
    "pos_tweets_set = []\n",
    "for tweet in happy:\n",
    "    pos_tweets_set.append((bag_of_words(tweet), 'pos'))    \n",
    "\n",
    "#all negative tweets in one list\n",
    "neg_tweets_set = []\n",
    "for tweet in sad:\n",
    "    neg_tweets_set.append((bag_of_words(tweet), 'neg'))\n",
    "\n",
    "#verify the length is the same as in the original one or no\n",
    "print (len(pos_tweets_set), len(neg_tweets_set)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données pour le modèle d’apprentissage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 8000\n"
     ]
    }
   ],
   "source": [
    "# choose the positive and negative tweets randomly\n",
    "from random import shuffle \n",
    "shuffle(pos_tweets_set)\n",
    "shuffle(neg_tweets_set)\n",
    "#split the data in two parts : test and train \n",
    "test_set = pos_tweets_set[:1000] + neg_tweets_set[:1000]\n",
    "train_set = pos_tweets_set[1000:] + neg_tweets_set[1000:]\n",
    "\n",
    "#size of test and train\n",
    "print(len(test_set),  len(train_set)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction d’un modèle d’apprentissage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7315\n",
      "Most Informative Features\n",
      "                     via = True              pos : neg    =     36.3 : 1.0\n",
      "                      aw = True              neg : pos    =     25.0 : 1.0\n",
      "                    glad = True              pos : neg    =     24.3 : 1.0\n",
      "                     bam = True              pos : neg    =     23.7 : 1.0\n",
      "                    cool = True              pos : neg    =     21.7 : 1.0\n",
      "                     x15 = True              neg : pos    =     21.0 : 1.0\n",
      "                    blog = True              pos : neg    =     18.3 : 1.0\n",
      "                     sad = True              neg : pos    =     17.7 : 1.0\n",
      "                opportun = True              pos : neg    =     15.0 : 1.0\n",
      "               goodnight = True              pos : neg    =     14.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    " \n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(accuracy) # Output: 0.765\n",
    " \n",
    "print (classifier.show_most_informative_features(10))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "<ProbDist with 2 samples>\n",
      "neg\n",
      "0.8795697043904147\n",
      "0.12043029560958547\n",
      "pos\n",
      "<ProbDist with 2 samples>\n",
      "pos\n",
      "0.030942659328954396\n",
      "0.9690573406710452\n"
     ]
    }
   ],
   "source": [
    "#test with a positive review\n",
    "twt_pos = \"it is bad . I hated the film. \"\n",
    "pos_tweet_set = bag_of_words(twt_pos)\n",
    "print (classifier.classify(pos_tweet_set)) \n",
    "\n",
    "pos_result = classifier.prob_classify(pos_tweet_set)\n",
    "print (pos_result) \n",
    "print (pos_result.max()) \n",
    "print (pos_result.prob(\"neg\")) \n",
    "print (pos_result.prob(\"pos\")) \n",
    " \n",
    " \n",
    "neg_tweet = \"It is a great film.I loved it.\"\n",
    "neg_tweet_set = bag_of_words(neg_tweet)\n",
    " \n",
    "print (classifier.classify(neg_tweet_set)) \n",
    "\n",
    " \n",
    "neg_result = classifier.prob_classify(neg_tweet_set)\n",
    "print (neg_result) \n",
    "print (neg_result.max()) \n",
    "print (neg_result.prob(\"neg\")) \n",
    "print (neg_result.prob(\"pos\")) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
